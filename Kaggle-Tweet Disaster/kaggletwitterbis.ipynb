{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "CbnCtyv1VsYE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "4V8gPAzuVsYH"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/train.csv\")\n",
    "X_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HqVRvh7_VsYI",
    "outputId": "3a4db2ec-cfe4-4f8f-eb49-d3f46f924efe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "4  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DZARCZUVsYJ",
    "outputId": "003a529c-c5c9-4626-b7c2-5b031663eff7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       #RockyFire Update => California Hwy. 20 closed...\n",
       "4       I'm on top of the hill and I can see a fire in...\n",
       "                              ...                        \n",
       "5341    Suicide bomber kills 15 in Saudi security site...\n",
       "5342    Two giant cranes holding a bridge collapse int...\n",
       "5343    @aria_ahrary @TheTawniest The out of control w...\n",
       "5344    Police investigating after an e-bike collided ...\n",
       "5345    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 2291, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['target']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQbgZrXhVsYK",
    "outputId": "a29a42dd-cb9b-46d0-f382-2e6976f2607f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8                                          What's up man?\n",
       "9                                           I love fruits\n",
       "10                                       Summer is lovely\n",
       "11                                      My car is so fast\n",
       "12                                 this is ridiculous....\n",
       "                              ...                        \n",
       "5325    @widda16 ... He's gone. You can relax. I thoug...\n",
       "5326     @jt_ruff23 @cameronhacker and I wrecked you both\n",
       "5327    Three days off from work and they've pretty mu...\n",
       "5328    @engineshed Great atmosphere at the British Li...\n",
       "5329    Cramer: Iger's 3 words that wrecked Disney's s...\n",
       "Name: text, Length: 3055, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['target']==0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3CwMx-JVsYK",
    "outputId": "7339c528-6088-4eee-e4aa-33895f1f62eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape(sin procesar):  (5346, 5)\n",
      "X_test.shape(sin procesar):  (2267, 4)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape(sin procesar): ', X_train.shape)\n",
    "print('X_test.shape(sin procesar): ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4WxVtSUVsYL",
    "outputId": "f891d2db-1965-4a67-ef06-049151099a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape(sin duplicados):  (5286, 5)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos los duplicados\n",
    "X_train = X_train.drop_duplicates(subset='text')\n",
    "print('X_train.shape(sin duplicados): ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzFsuxoltUN7",
    "outputId": "80251433-6bda-431a-c1f2-f8a04816d1e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     people receive wildfires evacuation orders in...\n",
       "1    just got sent this photo from ruby alaska as s...\n",
       "2    flood disaster heavy rain causes flash floodin...\n",
       "3    there's an emergency evacuation happening now ...\n",
       "4    i'm afraid that the tornado is coming to our area\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "signos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\>)|(\\=)|(\\<)\")\n",
    "signos_arroba = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\>)|(\\=)|(\\<)|(\\@)|(\\#)\")\n",
    "\n",
    "def signs_tweets(tweet):\n",
    "    return signos_arroba.sub('', tweet.lower())\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(signs_tweets)\n",
    "X_train['text'].head()\n",
    "X_test['text'] = X_test['text'].apply(signs_tweets)\n",
    "X_test['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to 'shelter in place' are ...\n",
      "3    rockyfire update  california hwy  closed in bo...\n",
      "4    i'm on top of the hill and i can see a fire in...\n",
      "Name: text, dtype: object\n",
      "0    people receive wildfires evacuation orders in ...\n",
      "1    just got sent this photo from ruby alaska as s...\n",
      "2    flood disaster heavy rain causes flash floodin...\n",
      "3    there's an emergency evacuation happening now ...\n",
      "4    i'm afraid that the tornado is coming to our area\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train['text'] = X_train['text'].str.strip()\n",
    "X_test['text'] = X_test['text'].str.strip()\n",
    "print(X_train['text'].head())\n",
    "print(X_test['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "ldH7sKX0VsYM"
   },
   "outputs": [],
   "source": [
    "def remove_links(df):\n",
    "    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(remove_links)\n",
    "X_test['text'] = X_test['text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "C3v0AEU8tnII",
    "outputId": "a242bcb9-bd2c-4631-c013-77573b56da56"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there's emergency evacuation happening buildin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i'm afraid tornado coming area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   6     NaN      NaN  people receive wildfires evacuation orders cal...\n",
       "1   7     NaN      NaN  got sent photo ruby alaska smoke wildfires pou...\n",
       "2  10     NaN      NaN  flood disaster heavy rain causes flash floodin...\n",
       "3  14     NaN      NaN  there's emergency evacuation happening buildin...\n",
       "4  15     NaN      NaN                     i'm afraid tornado coming area"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    return \" \".join([word for word in df.split() if word not in english_stopwords])\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(remove_stopwords)\n",
    "X_train.head()\n",
    "X_test['text'] = X_test['text'].apply(remove_stopwords)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "OMxzYszzVsYO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def english_stemmer(x):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return ' '.join([stemmer.stem(word) for word in x.split()])\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(english_stemmer)\n",
    "X_test['text'] = X_test['text'].apply(english_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nltk.download('wordnet')\\ndef get_lemmatized_text(corpus):\\n    cadena = '' \\n    from nltk.stem import WordNetLemmatizer\\n    lemmatizer = WordNetLemmatizer()\\n    for word in corpus.split():\\n        print(word)\\n        cadena = cadena + ' ' + lemmatizer.lemmatize(word)\\n    return cadena.strip()# Lematizamos las reviews\\nX_train['text'] = X_train['text'].apply(get_lemmatized_text)\\nX_test['text'] = X_test['text'].apply(get_lemmatized_text)\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nltk.download('wordnet')\n",
    "def get_lemmatized_text(corpus):\n",
    "    cadena = '' \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in corpus.split():\n",
    "        print(word)\n",
    "        cadena = cadena + ' ' + lemmatizer.lemmatize(word)\n",
    "    return cadena.strip()# Lematizamos las reviews\n",
    "X_train['text'] = X_train['text'].apply(get_lemmatized_text)\n",
    "X_test['text'] = X_test['text'].apply(get_lemmatized_text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "vrEtB_cjVsYP"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[['text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "XgBucXedVsYQ"
   },
   "outputs": [],
   "source": [
    "# Si solo es el modelo, no hará falta meterlo en un pipeline\n",
    "rand_forest = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('rand',RandomForestClassifier())\n",
    "])\n",
    "\n",
    "svm = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    (\"selectkbest\",SelectKBest()),\n",
    "    (\"svm\",SVC(probability=True))\n",
    "])\n",
    "\n",
    "reg_log = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    (\"imputer\",SimpleImputer()),\n",
    "    (\"reglog\",LogisticRegression())\n",
    "])\n",
    "\n",
    "gbc = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('gbc',GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "xgboost = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('xgboost',xgboost.XGBClassifier())\n",
    "])\n",
    "\n",
    "'''\n",
    "Para iterar hiperparámetros de varios elementos del pipeline, le ponemos un nombre\n",
    "a cada elemento en el pipeline, por ejemplo 'selectkbest' y 'svm', para luego en el\n",
    "grid de hiperparametros identificar sus respectivos parametros mediante el nombre\n",
    "que le hayamos puesto en el pipeline, dos guines bajos y el nombre del hiperparámetro.\n",
    "'''\n",
    "\n",
    "grid_random_forest = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)), # unigramas or bigramas\n",
    "    \"rand__n_estimators\": [120],\n",
    "    \"rand__max_depth\": [3,4,5,6,10,15,17],\n",
    "    \"rand__max_features\": [\"sqrt\", 3, 4]\n",
    "}\n",
    "\n",
    "grid_gradient_boosting = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)), # unigramas or bigramas\n",
    "    \"gbc__loss\": [\"deviance\"], # Deviance suele ir mejor.\n",
    "    \"gbc__learning_rate\": [0.05, 0.1, 0.2, 0.4, 0.5], # Cuanto más alto, mas aporta cada nuevo arbol\n",
    "    \"gbc__n_estimators\": [20,50,100,200], # Cuidado con poner muchos estiamdores ya que vamos a sobreajustar el modelo\n",
    "    \"gbc__max_depth\": [1,2,3,4,5], # No es necesario poner una profundiad muy alta. Cada nuevo arbol va corrigiendo el error de los anteriores.\n",
    "    \"gbc__max_features\": [\"sqrt\", 3, 4], # Igual que en el random forest\n",
    "}\n",
    "\n",
    "grid_xgboost = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)),# unigramas or bigramas\n",
    "    \"xgboost__learning_rate\": [0.05, 0.1, 0.2, 0.4, 0.5],# Cuanto más alto, mas aporta cada nuevo arbol\n",
    "    \"xgboost__n_estimators\": [20,50,100,200], # Cuidado con poner muchos estiamdores ya que vamos a\n",
    "                                                # sobreajustar el modelo\n",
    "    \"xgboost__max_depth\": [1,2,3,4,5] # No es necesario poner una profundiad muy alta. Cada nuevo\n",
    "                                        # arbol va corrigiendo el error de los anteriores.\n",
    "}\n",
    "\n",
    "grid_xgboost2 = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),# unigramas or bigramas\n",
    "    \"xgboost__learning_rate\": [0.2, 0.4, 0.5],# Cuanto más alto, mas aporta cada nuevo arbol\n",
    "    \"xgboost__n_estimators\": [100,200], # Cuidado con poner muchos estiamdores ya que vamos a\n",
    "                                                # sobreajustar el modelo\n",
    "    \"xgboost__max_depth\": [3,4,5] # No es necesario poner una profundiad muy alta. Cada nuevo\n",
    "                                        # arbol va corrigiendo el error de los anteriores.\n",
    "}\n",
    "\n",
    "svm_param = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)),# unigramas or bigramas\n",
    "    'selectkbest__k': [1,2,3],\n",
    "    'svm__C': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'svm__kernel': [\"linear\",\"poly\",\"rbf\"],\n",
    "    'svm__coef0': [-10.,-1., 0., 0.1, 0.5, 1, 10, 100],\n",
    "    'svm__gamma': ('scale', 'auto')\n",
    "}\n",
    "\n",
    "svm_param2 = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),# unigramas or bigramas\n",
    "    'selectkbest__k': [1,2,3],\n",
    "    'svm__C': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'svm__kernel': [\"linear\",\"poly\",\"rbf\"],\n",
    "    'svm__coef0': [0., 0.1, 0.5, 1, 10, 100],\n",
    "    'svm__gamma': ('scale', 'auto')\n",
    "}\n",
    "\n",
    "reg_log_param = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)),# unigramas or bigramas\n",
    "    \"imputer__strategy\": ['mean', 'median', 'most_frequent'],\n",
    "    \"reglog__penalty\": [\"l1\",\"l2\"],\n",
    "    \"reglog__C\": np.logspace(0, 4, 10)\n",
    "}\n",
    "\n",
    "reg_log_param2 = {\n",
    "    'vect__max_df': (1.9, 2.5),\n",
    "    'vect__min_df': (50, 75),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)),# unigramas or bigramas\n",
    "    \"imputer__strategy\": ['mean', 'median', 'most_frequent'],\n",
    "    \"reglog__penalty\": [\"l1\",\"l2\"],\n",
    "    \"reglog__C\": np.logspace(0, 4, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 756 candidates, totalling 2268 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2268 out of 2268 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Almaceno en una lista de tuplas los modelos (nombre que le pongo, el modelo, hiperparametros)\n",
    "models = [\n",
    "    ('rand_forest', rand_forest, grid_random_forest),\n",
    "    ('svm', svm, svm_param),\n",
    "    ('reg_log', reg_log, reg_log_param),\n",
    "    ('gradient_boosting', gbc, grid_gradient_boosting),\n",
    "    ('xgboost', xgboost, grid_xgboost)\n",
    "]\n",
    "\n",
    "model0 = [('rand_forest', rand_forest, grid_random_forest)]\n",
    "\n",
    "model1 = [('svm', svm, svm_param)]\n",
    "model1_1 = [('svm', svm, svm_param2)]\n",
    "\n",
    "model2 = [('reg_log', reg_log, reg_log_param)]\n",
    "model2_1 = [('reg_log', reg_log, reg_log_param2)]\n",
    "\n",
    "model3 = [('gradient_boosting', gbc, grid_gradient_boosting)]\n",
    "\n",
    "model4 = [('xgboost', xgboost, grid_xgboost)]\n",
    "model4_1 = [('xgboost', xgboost, grid_xgboost2)]\n",
    "\n",
    "# Declaro en un diccionario los pipelines e hiperparametros\n",
    "models_gridsearch = {}\n",
    "\n",
    "for i in model0:\n",
    "    models_gridsearch[i[0]] = GridSearchCV( i[1],\n",
    "                                            i[2],\n",
    "                                            cv = 3,\n",
    "                                            verbose =1,\n",
    "                                            scoring = 'roc_auc',\n",
    "                                            n_jobs = -1)\n",
    "    \n",
    "    models_gridsearch[i[0]].fit(X_train['text'], X_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "uLQk3r1SVsYR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rand_forest</td>\n",
       "      <td>0.7406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Grid  Best score\n",
       "0  rand_forest      0.7406"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grids = [(i, j.best_score_) for i, j in models_gridsearch.items()]\n",
    "\n",
    "best_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\n",
    "best_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 \treg_log \t0.708547\n",
    "[('reg_log',\n",
    "  {'imputer__strategy': 'mean',\n",
    "   'reglog__C': 1.0,\n",
    "   'reglog__penalty': 'l2',\n",
    "   'vect__max_df': 1.9,\n",
    "   'vect__max_features': 500,\n",
    "   'vect__min_df': 50,\n",
    "   'vect__ngram_range': (1, 2)})]\n",
    "   \n",
    "   \n",
    "reg_log \t0.710132 con snowball, sin espacios y sin arrobas y almohadillas   \n",
    "\n",
    "rand_forest \t0.732476 con lemmanización, sin espacios y sin arrobas y almohadillas\n",
    "\n",
    "rand_forest \t0.726502 con snowball, sin espacios y sin arrobas y almohadillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vectorizer2 = CountVectorizer(binary=True)\n",
    "\n",
    "vectorizer2.fit(X_train['text'])\n",
    "               \n",
    "X_train_baseline = vectorizer2.transform(X_train['text'])\n",
    "X_test_baseline = vectorizer2.transform(X_test['text'])\n",
    "                \n",
    "log_clf = LogisticRegression(C=1.0, penalty='l2')\n",
    "rnd_clf = RandomForestClassifier(n_estimators=90, max_depth=2, max_features=2)\n",
    "svm_clf = SVC(gamma='scale', probability=True, C=9.8, degree=0.0009)\n",
    "\n",
    "estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)]\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = estimators,\n",
    "                             voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(max_depth=2,\n",
       "                                                     max_features=2,\n",
       "                                                     n_estimators=90)),\n",
       "                             ('svc',\n",
       "                              SVC(C=9.8, degree=0.0009, probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train_baseline, X_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = voting_clf.predict_proba(X_test_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9561104805145668\n",
      "RandomForestClassifier 0.5741581536133182\n",
      "SVC 0.9907302307983352\n",
      "VotingClassifier 0.9903518728717366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train_baseline, X_train['target'])\n",
    "    y_pred = clf.predict(X_train_baseline)\n",
    "    print(clf.__class__.__name__, accuracy_score(X_train['target'], y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rand_forest',\n",
       "  Pipeline(steps=[('vect',\n",
       "                   CountVectorizer(max_df=1.9, max_features=1000, min_df=10)),\n",
       "                  ('rand',\n",
       "                   RandomForestClassifier(max_depth=15, max_features=3,\n",
       "                                          n_estimators=120))]))]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = [(i, j.best_estimator_) for i, j in models_gridsearch.items()]\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reg_log',\n",
       "  {'imputer__strategy': 'mean',\n",
       "   'reglog__C': 1.0,\n",
       "   'reglog__penalty': 'l2',\n",
       "   'vect__max_df': 1.9,\n",
       "   'vect__max_features': 500,\n",
       "   'vect__min_df': 50,\n",
       "   'vect__ngram_range': (1, 2)})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = [(i, j.best_params_) for i, j in models_gridsearch.items()]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = models_gridsearch[i[0]].predict_proba(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-180-012ada9007ed>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_submission['target'] = predictions_proba[:,-1]\n"
     ]
    }
   ],
   "source": [
    "df_submission = X_test[['id']]\n",
    "\n",
    "df_submission['target'] = predictions_proba[:,-1]\n",
    "\n",
    "df_submission.to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2247, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kaggletwitter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
